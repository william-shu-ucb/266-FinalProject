{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MjeEcAz4cpZ",
        "outputId": "248d7d73-45b0-47a9-9765-b03e262d2022"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ueWeRbq63aTc",
        "outputId": "016ca591-2be5-4775-e5cc-b8e601933a98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'true'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "# Disable W&B logging unless explicitly enabled\n",
        "os.environ.setdefault(\"WANDB_DISABLED\", \"true\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ROOT = Path('/content/drive/MyDrive/MIDS/266/FinalProject/Data/')\n",
        "MODEL_ROOT = Path('/content/drive/MyDrive/MIDS/266/FinalProject/Model/')"
      ],
      "metadata": {
        "id": "EO3awfdp3d3X"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_MODEL = \"microsoft/deberta-v3-base\"\n",
        "DEFAULT_MAX_LENGTH = 256\n",
        "DEFAULT_EPOCHS = 1\n",
        "DEFAULT_BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "3q0kDj9z3xyX"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_split(pair_type: str, split: str) -> pd.DataFrame:\n",
        "    path = DATA_ROOT / pair_type / f\"{split}.jsonl\"\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Missing split file: {path}\")\n",
        "    return pd.read_json(path, lines=True)"
      ],
      "metadata": {
        "id": "zGw-5Pg13y7D"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PairDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, tokenizer, field1: str, field2: str, max_length: int):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.field1 = field1\n",
        "        self.field2 = field2\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        row = self.df.iloc[idx]\n",
        "        encoded = self.tokenizer(\n",
        "            str(row[self.field1]),\n",
        "            str(row[self.field2]),\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": torch.tensor(int(row[\"label\"]), dtype=torch.long),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "ItMkZTCK30Dx"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred) -> Dict[str, float]:\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds),\n",
        "    }"
      ],
      "metadata": {
        "id": "zZ7f6i8_32Ai"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_deberta(\n",
        "    pair_type: str,\n",
        "    field1: str,\n",
        "    field2: str,\n",
        "    save_name: str,\n",
        "    model_name: str = DEFAULT_MODEL,\n",
        "    max_length: int = DEFAULT_MAX_LENGTH,\n",
        "    epochs: int = DEFAULT_EPOCHS,\n",
        "    batch_size: int = DEFAULT_BATCH_SIZE,\n",
        "    learning_rate: float = 2e-5,\n",
        ") -> str:\n",
        "    train_df = load_split(pair_type, \"train\")\n",
        "    val_df = load_split(pair_type, \"val\")\n",
        "\n",
        "    MODEL_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "    save_path = MODEL_ROOT / save_name\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    train_ds = PairDataset(train_df, tokenizer, field1, field2, max_length)\n",
        "    val_ds = PairDataset(val_df, tokenizer, field1, field2, max_length)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=str(save_path / \"checkpoints\"),\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=1,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        report_to=\"none\",\n",
        "        logging_steps=50,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Save best model and tokenizer\n",
        "    trainer.save_model(str(save_path))\n",
        "    tokenizer.save_pretrained(str(save_path))\n",
        "    return str(save_path)"
      ],
      "metadata": {
        "id": "ahxFduY93802"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_deberta(\n",
        "    model_path: Path,\n",
        "    test_df: pd.DataFrame,\n",
        "    field1: str,\n",
        "    field2: str,\n",
        "    max_length: int,\n",
        "    batch_size: int,\n",
        "    model_name: str = DEFAULT_MODEL,\n",
        ") -> Dict[str, float]:\n",
        "    config = AutoConfig.from_pretrained(str(model_path))\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(str(model_path), config=config)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    test_ds = PairDataset(test_df, tokenizer, field1, field2, max_length)\n",
        "\n",
        "    eval_args = TrainingArguments(\n",
        "        output_dir=str(model_path / \"eval\"),\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        report_to=\"none\",\n",
        "        fp16=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=eval_args,\n",
        "        eval_dataset=test_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    metrics = trainer.evaluate()\n",
        "    return {\n",
        "        \"test_loss\": metrics.get(\"eval_loss\", float(\"nan\")),\n",
        "        \"test_accuracy\": metrics.get(\"eval_accuracy\", float(\"nan\")),\n",
        "        \"test_f1\": metrics.get(\"eval_f1\", float(\"nan\")),\n",
        "    }"
      ],
      "metadata": {
        "id": "1d0x6cjD3_yu"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_deberta_pipeline(\n",
        "    pair_type: str,\n",
        "    field1: str,\n",
        "    field2: str,\n",
        "    save_name: str,\n",
        "    model_name: str = DEFAULT_MODEL,\n",
        "    max_length: int = DEFAULT_MAX_LENGTH,\n",
        "    epochs: int = DEFAULT_EPOCHS,\n",
        "    batch_size: int = DEFAULT_BATCH_SIZE,\n",
        "    train_first: bool = True,\n",
        ") -> Dict[str, float]:\n",
        "    if train_first:\n",
        "        print(f\"Training DeBERTa on {pair_type} -> {save_name}\")\n",
        "        model_path = Path(\n",
        "            train_deberta(\n",
        "                pair_type=pair_type,\n",
        "                field1=field1,\n",
        "                field2=field2,\n",
        "                save_name=save_name,\n",
        "                model_name=model_name,\n",
        "                max_length=max_length,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        model_path = MODEL_ROOT / save_name\n",
        "        if not model_path.exists():\n",
        "            raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
        "        print(f\"Skipping training, loading DeBERTa from {model_path}\")\n",
        "\n",
        "    test_df = load_split(pair_type, \"test\")\n",
        "    metrics = evaluate_deberta(\n",
        "        model_path=model_path,\n",
        "        test_df=test_df,\n",
        "        field1=field1,\n",
        "        field2=field2,\n",
        "        max_length=max_length,\n",
        "        batch_size=batch_size,\n",
        "        model_name = model_name,\n",
        "    )\n",
        "    print(f\"Test metrics for {pair_type}: acc={metrics['test_accuracy']:.4f}, f1={metrics['test_f1']:.4f}\")\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "ZAdpXY154Brr"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_deberta_pipeline(\n",
        "    pair_type=\"title-title-pair\",\n",
        "    field1=\"title1\",\n",
        "    field2=\"title2\",\n",
        "    save_name=\"deberta-title-v3-base\",\n",
        "    max_length=128,\n",
        "    epochs=1,\n",
        "    batch_size=32,\n",
        "    train_first=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qF6ucX34EnB",
        "outputId": "74d78115-ab83-4f1e-886c-cfd03d17c42a"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping training, loading DeBERTa from /content/drive/MyDrive/MIDS/266/FinalProject/Model/deberta-title-v3-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1313487830.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.0769580826163292, 'eval_model_preparation_time': 0.0029, 'eval_accuracy': 0.9752430439155213, 'eval_f1': 0.9752617033749267, 'eval_runtime': 95.34, 'eval_samples_per_second': 625.76, 'eval_steps_per_second': 19.562}\n",
            "Test metrics for title-title-pair: acc=0.9752, f1=0.9753\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.0769580826163292,\n",
              " 'test_accuracy': 0.9752430439155213,\n",
              " 'test_f1': 0.9752617033749267}"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_deberta_pipeline(\n",
        "    pair_type=\"body-body-pair\",\n",
        "    field1=\"body1\",\n",
        "    field2=\"body2\",\n",
        "    save_name=\"deberta-body-v3-base\",\n",
        "    max_length=256,\n",
        "    epochs=1,\n",
        "    batch_size=12,\n",
        "    train_first=False,\n",
        ")"
      ],
      "metadata": {
        "id": "gcp_mwaQ4Hsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9735f4ec-d50f-4ae6-88f2-90dff369a588"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping training, loading DeBERTa from /content/drive/MyDrive/MIDS/266/FinalProject/Model/deberta-body-v3-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1313487830.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.0740351751446724, 'eval_model_preparation_time': 0.003, 'eval_accuracy': 0.9820656783939896, 'eval_f1': 0.9821313586606568, 'eval_runtime': 243.0305, 'eval_samples_per_second': 203.736, 'eval_steps_per_second': 16.981}\n",
            "Test metrics for body-body-pair: acc=0.9821, f1=0.9821\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.0740351751446724,\n",
              " 'test_accuracy': 0.9820656783939896,\n",
              " 'test_f1': 0.9821313586606568}"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_deberta_pipeline(\n",
        "    pair_type=\"post-post-pair\",\n",
        "    field1=\"post1\",\n",
        "    field2=\"post2\",\n",
        "    save_name=\"deberta-post-v3-base\",\n",
        "    max_length=256,\n",
        "    epochs=1,\n",
        "    batch_size=12,\n",
        "    train_first=False,\n",
        ")"
      ],
      "metadata": {
        "id": "yNK2v8T24JXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ecf32ce-3685-4aab-8920-a4b66389af32"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping training, loading DeBERTa from /content/drive/MyDrive/MIDS/266/FinalProject/Model/deberta-post-v3-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1313487830.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.038536615669727325, 'eval_model_preparation_time': 0.0029, 'eval_accuracy': 0.9912482726853984, 'eval_f1': 0.9912428944538332, 'eval_runtime': 267.0531, 'eval_samples_per_second': 195.107, 'eval_steps_per_second': 16.259}\n",
            "Test metrics for post-post-pair: acc=0.9912, f1=0.9912\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.038536615669727325,\n",
              " 'test_accuracy': 0.9912482726853984,\n",
              " 'test_f1': 0.9912428944538332}"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    }
  ]
}
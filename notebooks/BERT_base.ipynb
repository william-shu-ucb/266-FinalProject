{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# BERT-base"],"metadata":{"id":"bsnn-m1uK4lk"}},{"cell_type":"code","source":["!pip install transformers datasets accelerate -q\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import TrainingArguments, Trainer\n","import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score\n","import torch\n","from torch.utils.data import Dataset\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","# ----------------------------\n","# Dataclass set and tokeniztaion\n","# ----------------------------\n","class PairDataset(Dataset):\n","    def __init__(self, df, tokenizer, field1, field2, max_length=128):\n","        self.df = df\n","        self.tokenizer = tokenizer\n","        self.field1 = field1\n","        self.field2 = field2\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        encoded = self.tokenizer(\n","            row[self.field1],\n","            row[self.field2],\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=self.max_length,\n","            return_tensors=\"pt\",\n","        )\n","        return {\n","            \"input_ids\": encoded[\"input_ids\"].squeeze(),\n","            \"attention_mask\": encoded[\"attention_mask\"].squeeze(),\n","            \"labels\": torch.tensor(row[\"label\"], dtype=torch.long),\n","        }\n","\n","\n","# ----------------------------\n","# metirc help function\n","# ----------------------------\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = np.argmax(logits, axis=-1)\n","    return {\n","        \"accuracy\": accuracy_score(labels, preds),\n","        \"f1\": f1_score(labels, preds),\n","    }\n","\n","\n","# ----------------------------\n","# train_and_save help function\n","# ----------------------------\n","def train_and_save_bert(train_df, val_df, field1, field2, save_name,\n","                        max_length=128, epochs=1, batch=16):\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","    train_dataset = PairDataset(train_df, tokenizer, field1, field2, max_length)\n","    val_dataset   = PairDataset(val_df, tokenizer, field1, field2, max_length)\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        \"bert-base-uncased\",\n","        num_labels=2\n","    )\n","\n","    training_args = TrainingArguments(\n","        output_dir=\"./bert-base-checkpoints\",\n","        eval_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        learning_rate=2e-5,\n","        per_device_train_batch_size=batch,\n","        per_device_eval_batch_size=batch,\n","        num_train_epochs=epochs,\n","        weight_decay=0.01,\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"f1\",\n","        fp16=True,\n","        report_to=\"none\"\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    trainer.train()\n","\n","    # Save BEST model\n","    save_path = f\"/content/drive/MyDrive/266NoteBooks/FinalProject/Model/{save_name}\"\n","    trainer.save_model(save_path)\n","    tokenizer.save_pretrained(save_path)\n","\n","    print(\"Model saved to:\", save_path)\n","\n","    return save_path\n","\n","# ----------------------------\n","# load_and_eval help function\n","# ----------------------------\n","def load_and_eval_bert(model_path, test_df, field1, field2, max_length=128):\n","    tokenizer = AutoTokenizer.from_pretrained(model_path)\n","    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n","\n","    test_dataset = PairDataset(test_df, tokenizer, field1, field2, max_length)\n","\n","    trainer = Trainer(\n","        model=model,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    raw = trainer.evaluate(test_dataset)\n","\n","    metrics = {\n","        \"test_loss\": raw[\"eval_loss\"],\n","        \"test_accuracy\": raw[\"eval_accuracy\"],\n","        \"test_f1\": raw[\"eval_f1\"],\n","    }\n","\n","    print(\"\\nTest Metrics:\", metrics)\n","    return metrics\n","\n","\n","# ----------------------------\n","# Run pipeline (train-save-load-eval OR load-eval)\n","# ----------------------------\n","def run_bert_pipeline(\n","        pair_type, field1, field2, save_name,\n","        max_length=128, epochs=1, batch=16,\n","        train_first=True):  # turn on training mode, if false direstly load from saved version\n","\n","    data_root = '/content/drive/MyDrive/266NoteBooks/FinalProject/Data/'\n","    model_root = \"/content/drive/MyDrive/266NoteBooks/FinalProject/Model\"\n","    model_path = f\"{model_root}/{save_name}\"\n","\n","    # ----------------------------\n","    # Load Data\n","    # ----------------------------\n","    train_df = pd.read_json(f\"{data_root}{pair_type}/train.jsonl\", lines=True)\n","    val_df   = pd.read_json(f\"{data_root}{pair_type}/val.jsonl\", lines=True)\n","    test_df  = pd.read_json(f\"{data_root}{pair_type}/test.jsonl\", lines=True)\n","\n","    print(f\"\\n Loaded dataset: {pair_type}\")\n","    print(\"Train:\", train_df.shape, \"Val:\", val_df.shape, \"Test:\", test_df.shape)\n","\n","    # ----------------------------\n","    # Option 1: Train + Save\n","    # ----------------------------\n","    if train_first:\n","        print(f\"\\n Training & Saving Model: {save_name}\")\n","        model_path = train_and_save_bert(\n","            train_df=train_df,\n","            val_df=val_df,\n","            field1=field1,\n","            field2=field2,\n","            save_name=save_name,\n","            max_length=max_length,\n","            epochs=epochs,\n","            batch=batch\n","        )\n","    else:\n","        print(f\"\\n Skipping training. Loading existing model: {model_path}\")\n","\n","    # ----------------------------\n","    # Load & Evaluate\n","    # ----------------------------\n","    print(f\"\\n Evaluating model: {save_name}\")\n","    metrics = load_and_eval_bert(\n","        model_path=model_path,\n","        test_df=test_df,\n","        field1=field1,\n","        field2=field2,\n","        max_length=max_length\n","    )\n","\n","    print(f\"\\n Finished: {save_name}\")\n","    # print(metrics)\n","    return metrics\n","\n"],"metadata":{"id":"iAmFB4-ZtnlO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the max token length of title, body, and post to decide max_length\n","# The max token limit for BERT is 512（ ModernBERT can take 4k， will try later)\n","def get_length_stats(df, field1, field2):\n","  from transformers import AutoTokenizer\n","  tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","  lengths = []\n","  for i, row in df.iterrows():\n","      text1 = str(row[field1])\n","      text2 = str(row[field2])\n","      encoded = tokenizer.encode(text1, text2, add_special_tokens=True)\n","      lengths.append(len(encoded))\n","  lengths = np.array(lengths)\n","  return {\n","      \"mean\": float(np.mean(lengths)),\n","      \"median\": float(np.median(lengths)),\n","      \"95th_percentile\": float(np.percentile(lengths, 95)),\n","      \"max\": int(np.max(lengths)),\n","      \"min\": int(np.min(lengths)),\n","  }\n","MydriveRootPath = '/content/drive/MyDrive/266NoteBooks/FinalProject/Data/'\n","train_title = pd.read_json(f\"{MydriveRootPath}title-title-pair/train.jsonl\", lines=True)\n","stats_title = get_length_stats(train_title, \"title1\", \"title2\")\n","train_body = pd.read_json(f\"{MydriveRootPath}body-body-pair/train.jsonl\", lines=True)\n","stats_body = get_length_stats(train_body, \"body1\", \"body2\")\n","train_post = pd.read_json(f\"{MydriveRootPath}post-post-pair/train.jsonl\", lines=True)\n","stats_post = get_length_stats(train_post, \"post1\", \"post2\")\n","print(stats_title)\n","print(stats_body)\n","print(stats_post)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UYq9lto12puU","outputId":"611dcbb4-6a6f-4b04-bba1-1758e5a86b69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1388 > 512). Running this sequence through the model will result in indexing errors\n","Token indices sequence length is longer than the specified maximum sequence length for this model (568 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["{'mean': 30.999358712523488, 'median': 28.0, '95th_percentile': 57.0, 'max': 216, 'min': 7}\n","{'mean': 344.0520482478365, 'median': 270.0, '95th_percentile': 854.0, 'max': 3307, 'min': 20}\n","{'mean': 372.38553132675077, 'median': 298.0, '95th_percentile': 888.0, 'max': 3730, 'min': 38}\n"]}]},{"cell_type":"code","source":["# Run BERT on title data\n","run_bert_pipeline(\"title-title-pair\", \"title1\", \"title2\", \"bert-base-title-v1\",\n","                  max_length=64, epochs=1, batch=32, train_first=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":578},"id":"A5yfG0SywP_7","outputId":"755cf441-68e2-4ff1-96ff-fed80eed21f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Loaded dataset: title-title-pair\n","Train: (489640, 3) Val: (58976, 3) Test: (59660, 3)\n","\n"," Training & Saving Model → bert-base-title-v1\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/tmp/ipython-input-2769880021.py:84: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='15302' max='15302' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15302/15302 33:26, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.108800</td>\n","      <td>0.130025</td>\n","      <td>0.954236</td>\n","      <td>0.953749</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2769880021.py:113: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]},{"output_type":"stream","name":"stdout","text":["Model saved to: /content/drive/MyDrive/266NoteBooks/FinalProject/Model/bert-base-title-v1\n","\n"," Evaluating model: bert-base-title-v1\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='7458' max='7458' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7458/7458 02:06]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Test Evaluation: {'eval_loss': 0.119511678814888, 'eval_model_preparation_time': 0.0027, 'eval_accuracy': 0.9571237009721757, 'eval_f1': 0.9567057071288335, 'eval_runtime': 126.7469, 'eval_samples_per_second': 470.702, 'eval_steps_per_second': 58.842}\n","\n"," Finished: bert-base-title-v1\n","{'eval_loss': 0.119511678814888, 'eval_model_preparation_time': 0.0027, 'eval_accuracy': 0.9571237009721757, 'eval_f1': 0.9567057071288335, 'eval_runtime': 126.7469, 'eval_samples_per_second': 470.702, 'eval_steps_per_second': 58.842}\n"]},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.119511678814888,\n"," 'eval_model_preparation_time': 0.0027,\n"," 'eval_accuracy': 0.9571237009721757,\n"," 'eval_f1': 0.9567057071288335,\n"," 'eval_runtime': 126.7469,\n"," 'eval_samples_per_second': 470.702,\n"," 'eval_steps_per_second': 58.842}"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["# Run BERT on body data\n","run_bert_pipeline(\"body-body-pair\", \"body1\", \"body2\", \"bert-base-body-v1\",\n","                  max_length=512, epochs=1, batch=12, train_first=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":578},"id":"KBBSmI4SzOiw","outputId":"25be4f80-c558-4f95-a2e5-105a2eb18724"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Loaded dataset: body-body-pair\n","Train: (402588, 3) Val: (48390, 3) Test: (49514, 3)\n","\n"," Training & Saving Model → bert-base-body-v1\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/tmp/ipython-input-2769880021.py:84: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='33549' max='33549' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [33549/33549 3:07:12, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.064400</td>\n","      <td>0.096891</td>\n","      <td>0.976090</td>\n","      <td>0.976033</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Model saved to: /content/drive/MyDrive/266NoteBooks/FinalProject/Model/bert-base-body-v1\n","\n"," Evaluating model: bert-base-body-v1\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2769880021.py:113: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6190' max='6190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6190/6190 07:08]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Test Evaluation: {'eval_loss': 0.08460821956396103, 'eval_model_preparation_time': 0.0029, 'eval_accuracy': 0.9786525023225754, 'eval_f1': 0.978651208822282, 'eval_runtime': 428.8374, 'eval_samples_per_second': 115.461, 'eval_steps_per_second': 14.434}\n","\n"," Finished: bert-base-body-v1\n","{'eval_loss': 0.08460821956396103, 'eval_model_preparation_time': 0.0029, 'eval_accuracy': 0.9786525023225754, 'eval_f1': 0.978651208822282, 'eval_runtime': 428.8374, 'eval_samples_per_second': 115.461, 'eval_steps_per_second': 14.434}\n"]},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.08460821956396103,\n"," 'eval_model_preparation_time': 0.0029,\n"," 'eval_accuracy': 0.9786525023225754,\n"," 'eval_f1': 0.978651208822282,\n"," 'eval_runtime': 428.8374,\n"," 'eval_samples_per_second': 115.461,\n"," 'eval_steps_per_second': 14.434}"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# Run BERT on post data\n","run_bert_pipeline(\"post-post-pair\", \"post1\", \"post2\", \"bert-base-post-v1\",\n","                   max_length=512, epochs=1, batch=8, train_first=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":578},"id":"rvhX0xOq2jAi","outputId":"441f26cd-a874-477b-8503-da881cdd754b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Loaded dataset: post-post-pair\n","Train: (402276, 3) Val: (46504, 3) Test: (52104, 3)\n","\n"," Training & Saving Model → bert-base-post-v1\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/tmp/ipython-input-2769880021.py:84: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='50285' max='50285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [50285/50285 3:19:21, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.054800</td>\n","      <td>0.059949</td>\n","      <td>0.986840</td>\n","      <td>0.986810</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2769880021.py:113: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]},{"output_type":"stream","name":"stdout","text":["Model saved to: /content/drive/MyDrive/266NoteBooks/FinalProject/Model/bert-base-post-v1\n","\n"," Evaluating model: bert-base-post-v1\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6513' max='6513' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6513/6513 07:35]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Test Evaluation: {'eval_loss': 0.06528539955615997, 'eval_model_preparation_time': 0.0028, 'eval_accuracy': 0.9859319821894672, 'eval_f1': 0.9859062854505951, 'eval_runtime': 455.476, 'eval_samples_per_second': 114.395, 'eval_steps_per_second': 14.299}\n","\n"," Finished: bert-base-post-v1\n","{'eval_loss': 0.06528539955615997, 'eval_model_preparation_time': 0.0028, 'eval_accuracy': 0.9859319821894672, 'eval_f1': 0.9859062854505951, 'eval_runtime': 455.476, 'eval_samples_per_second': 114.395, 'eval_steps_per_second': 14.299}\n"]},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.06528539955615997,\n"," 'eval_model_preparation_time': 0.0028,\n"," 'eval_accuracy': 0.9859319821894672,\n"," 'eval_f1': 0.9859062854505951,\n"," 'eval_runtime': 455.476,\n"," 'eval_samples_per_second': 114.395,\n"," 'eval_steps_per_second': 14.299}"]},"metadata":{},"execution_count":45}]}]}